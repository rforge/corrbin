{
    "contents" : "library(mlogit)\nlibrary(mvtnorm)\nlibrary(dplyr)\n\ngen.pmatrix <- function(baseline.p, logit.trend, nc){\n  #generate matrix with probability distribution of outcomes in each column\n  nr <- length(baseline.p)\n  logit.baseline <- log(baseline.p/(1-baseline.p))\n  logit.trends <- rep(logit.trend, length.out=nr-1)\n  free.rows <- 1:(nr-1)\n  #calculate probability matrix\n  pmat <- matrix(0, nr=nr, nc=nc)\n  for (j in 1:nc){\n    pmat[free.rows,j] <- baseline.p[free.rows] *exp(logit.trends*(j-1))\n    adj.sum <- sum(pmat[free.rows,j])\n    pmat[free.rows,j] <- pmat[free.rows,j] / (baseline.p[nr] + adj.sum)\n  }  \n  pmat[nr,] <- 1 - colSums(pmat[free.rows,,drop=FALSE])\n  pmat\n}\n\ngen.pmatrix.linear <- function(baseline.p, end.p=baseline.p, nc){\n  #generate matrix with probability distribution of outcomes in each column\n  # with linear slope of probabilities from baseline to end\n  if (abs(sum(baseline.p) - 1) > 1e-10) stop(\"Baseline probabilities should add up to 1\")\n  if (abs(sum(end.p) - 1) > 1e-10) stop(\"End probabilities should add up to 1\")\n  nr <- length(baseline.p)\n  if (length(end.p) != nr) stop(\"Baseline and end vectors should have the same length\")\n  pmat <- matrix(baseline.p, nrow=nr, ncol=nc, byrow=FALSE)\n  slope <- (end.p - baseline.p) / (nc-1)\n  pmat <- pmat + slope * (col(pmat) - 1)  \n  pmat\n}\n\ngen.data <- function(nvec, pmat){\n  #generate counts, conditional on group sizes nvec\n  res <- sapply(seq_along(nvec), function(i){\n    rmultinom(1, size=nvec[i], prob=pmat[,i])})\n  rownames(res) <- 1:nrow(res)\n  res  \n}\n\nCA.test <- function(x, scores = 1:nrow(x)){\n  x <- t(x)\n  if (dim(x)[2]!=2){stop(\"Cochran-Armitage test for trend must be  used with a (2,R) table\",call.=FALSE) }\n  \n  nidot <- apply(x,1,sum)\n  n <- sum(nidot)\n  \n  Ri <- scores\n  Rbar <- sum(nidot*Ri)/n\n  \n  s2 <- sum(nidot*(Ri-Rbar)^2)\n  pdot1 <- sum(x[,1])/n\n  if (pdot1==0 | pdot1==1) return(list(statistic=0, p.value=1))\n  Tt <- sum(x[,1]*(Ri-Rbar))/sqrt(pdot1*(1-pdot1)*s2)\n  p.value.uni <- 1-pnorm(abs(Tt))\n  p.value.bi <- 2*p.value.uni\n  \n  out <- list(statistic=Tt, p.value=p.value.bi)\n  return(out)\n}\n\nmultiCA.matrices <- function(pmat, cvec, nvec){\n  # calculates mean/var matrices under H0 and H1 for theoretical experimentation\n  # pmat -  probability distribution of outcomes in each column\n  # cvec - scores\n  # nvec - group sample sizes\n  nc <- ncol(pmat)\n  nr <- nrow(pmat)\n  N <- sum(nvec)\n  \n  phat <- as.vector(pmat %*% nvec) / N\n  cbar <- as.vector(crossprod(nvec, cvec)) / N\n  dvec <- sqrt(nvec) * (cvec - cbar)\n  s2 <- as.vector(crossprod(dvec, dvec))\n  \n  Sigma0 <- s2 * (diag(phat) - phat %*% t(phat))\n  A1 <- diag(nrow=nr) - matrix(1/nc, nrow=nr, ncol=nr) \n  Sigma0inv <- (A1 %*% diag(1/phat) %*% A1) / s2\n  \n  p.star <- pmat %*% diag(dvec)\n  Mu <- as.vector(pmat %*% (nvec * (cvec - cbar)))\n  Sigma <- diag(as.vector(p.star %*% dvec))  - tcrossprod(p.star)\n  \n  #calculations without last category, because one eigenvalue is 0\n  A <- solve(Sigma0[-nr,-nr])\n  S <- Sigma[-nr,-nr]\n  C <- chol(S)\n  ee <- eigen(C %*% A %*% t(C))\n  lambda <- ee$values\n  P <- t(ee$vectors)\n  nu <- as.vector(P %*% solve(t(C)) %*% Mu[-nr])\n  ncp <- nu^2\n  \n  list(Sigma0 = Sigma0, Sigma0inv = Sigma0inv, Mu = Mu, Sigma = Sigma,\n       lambda=lambda, ncp=ncp)\n}\n\nmultiCA.Xvector <- function(x, cvec){\n  # returns vector of jointly normal CA statistics\n  N <- sum(x)\n  nvec <- colSums(x)\n  cbar <- as.vector(crossprod(nvec, cvec)) / N\n  X <- x %*% (cvec - cbar)\n  as.vector(X)\n}\n\nmultiCA.test <- function(x, scores=1:ncol(x)){\n  CAT <- numeric(nrow(x))\n  CAp <- numeric(nrow(x))\n  p <- numeric(nrow(x))\n  #for adjusted p-values\n  for (j in 1:nrow(x)){\n    x.collapse <- rbind(x[j,], colSums(x[-j,,drop=FALSE]))\n    ca <- CA.test(x.collapse, scores=scores)\n    CAT[j] <- ca$statistic\n    CAp[j] <- ca$p.value\n    p[j] <- sum(x[j,])/sum(x)\n  }\n  \n  #overall statistic\n  Tt <- sum(CAT^2 * (1-p))\n  p.value <- pchisq(Tt, df=nrow(x)-1, lower=FALSE)\n  \n  res <- list(statistic = Tt, p.value=p.value, CA.statistics = CAT, CA.p.values=CAp)\n  res\n}\n\npartial.multiCA.test <- function(x, outcomes=1:nrow(x), scores=1:ncol(x)){\n  #x has rows - outcomes, columns - groups (with a trend)\n  #outcomes has the indices (or names if x has rownames) of the outcomes to be tested\n  \n  K <- nrow(x)\n  full <- length(outcomes) == K  #full test\n  \n  nidot <- apply(x, 2, sum)\n  n <- sum(nidot)\n  \n  cbar <- sum(nidot * scores)/n\n  \n  s2 <- sum(nidot * (scores - cbar)^2)\n  pdot <- prop.table(rowSums(x))[outcomes]\n  nonz <- (pdot > 0)\n  \n  if (!any(nonz)) return(1)\n  \n  X <- x[outcomes, ] %*% (scores - cbar)\n  \n  if (full || sum(pdot) >= 1){\n    Tt <- ( sum(X[nonz]^2 / pdot[nonz])) / s2\n  } else {\n    Tt <- (sum(X)^2 / (1-sum(pdot)) + sum(X[nonz]^2 / pdot[nonz])) / s2\n  }\n  \n  p.value <- pchisq(Tt, df=length(outcomes) - full, lower=FALSE)\n  return(p.value)  \n}\n\nmultiKruskal <- function(x){\n  xd <- as.data.frame.table(x)\n  xd2 <- xd[rep(1:nrow(xd), xd$Freq), ]\n  xd2$Freq <- NULL\n  res <- kruskal.test(Var2 ~ Var1, data=xd2)\n  res\n}\n\n# combine multiple M-W tests\nmultiMannWhitney <- function(x){\n  xd <- as.data.frame.table(x)\n  xd2 <- xd[rep(1:nrow(xd), xd$Freq), ]\n  xd2$Freq <- NULL\n  \n  p0 <- prop.table(rowSums(x))\n  mwstats <- numeric(nrow(x))\n  names(mwstats) <- unique(xd2$Var1)\n  \n  for (gr in unique(xd2$Var1)){\n    x <- with(xd2, unclass(Var2)[Var1 == gr])\n    y <- with(xd2, unclass(Var2)[Var1 != gr])\n    # from wilcox.test.default\n    r <- rank(c(x, y))\n    n.x <- as.double(length(x))\n    n.y <- as.double(length(y))\n    STATISTIC <- c(W = sum(r[seq_along(x)]) - n.x * (n.x + 1)/2)\n    NTIES <- table(r)\n    z <- STATISTIC - n.x * n.y/2\n    SIGMA <- sqrt((n.x * n.y/12) * ((n.x + n.y + 1) - \n                                      sum(NTIES^3 - NTIES)/((n.x + n.y) * (n.x + n.y - 1))))\n    z <- z / SIGMA\n    mwstats[gr] <- z\n  }\n  stat <- sum(mwstats^2 * (1-p0))\n  list(statistic = stat, mwstats=mwstats)\n}\n\nmultiScore <- function(x){\n  xd <- as.data.frame.table(x)\n  xd2 <<- xd[rep(1:nrow(xd), xd$Freq), ]; xd2$Freq <<- NULL; rownames(xd2) <<- NULL\n  on.exit(rm(xd2, envir = .GlobalEnv))\n  m1 <- mlogit(Var1 ~ 0 | unclass(Var2), data=xd2, shape=\"wide\", choice=\"Var1\")\n  m0 <- mlogit(Var1 ~ 0 | 1, data=xd2, shape=\"wide\", choice=\"Var1\")\n  scoretest(m0, m1)\n}\n\n\n# power of overall test statistic\nmultiCA.power <- function(pmat, nvec, scores=1:ncol(pmat), sig.level=0.05){\n  N <- sum(nvec)\n  p0 <- as.vector(pmat %*% nvec) / N\n  cbar <- as.vector(crossprod(nvec, scores)) / N\n  dvec <- sqrt(nvec) * (scores - cbar)\n  s2 <- as.vector(crossprod(dvec, dvec))\n  slopes <- apply(pmat, 1, function(x)coef(lm(x ~ scores, weight=nvec))[\"scores\"])\n  \n  ncp <- sum(slopes^2 / p0) * s2\n  df <- nrow(pmat) - 1\n  crit <- qchisq(1-sig.level, df=df)\n  pwr <- pchisq(crit, df=df, ncp=ncp, lower=FALSE)\n  pwr\n}\n# effect size (ncp)\nmultiCA.ncp <- function(pmat, nvec, scores=1:ncol(pmat)){\n  N <- sum(nvec)\n  p0 <- as.vector(pmat %*% nvec) / N\n  cbar <- as.vector(crossprod(nvec, scores)) / N\n  dvec <- sqrt(nvec) * (scores - cbar)\n  s2 <- as.vector(crossprod(dvec, dvec))\n  slopes <- apply(pmat, 1, function(x)coef(lm(x ~ scores, weight=nvec))[\"scores\"])\n  \n  ncp <- sum(slopes^2 / p0) * s2\n  ncp\n}\n\nsimulate.test <- function(test.fun, nvec, pmat, R=500, sig.level=0.05){\n  tt <- pvals <- numeric(R)\n  for (i in 1:R){\n    x <- gen.data(nvec=nvec, pmat=pmat)\n    xres <- test.fun(x)\n    tt[i] <- xres$statistic\n    pvals[i] <- xres$p.value\n  }\n  pwr <- mean(pvals <= sig.level)\n  invisible(list(power=pwr, statistics=tt, p.values=pvals))\n}\n\n# power of individual tests (+ overall)\nsimulate.test.indiv <- function(test.fun, nvec, pmat, R=500, sig.level=0.05,\n                                indiv.name=\"CA.adj.pvalues\"){\n  tt <- pvals <- numeric(R)\n  ipvals <- matrix(NA, ncol=nrow(pmat), nrow=R)\n  for (i in 1:R){\n    x <- gen.data(nvec=nvec, pmat=pmat)\n    xres <- test.fun(x)\n    tt[i] <- xres$statistic\n    pvals[i] <- xres$p.value\n    ipvals[i, ] <- xres[[indiv.name]]\n  }\n  pwr <- mean(pvals <= sig.level)\n  minppwr <- mean(apply(ipvals, 1, min) <= sig.level)\n  indiv.pwr <- colMeans(ipvals <= sig.level)\n  invisible(list(power=pwr, minp.power=minppwr, indiv.power=indiv.pwr,\n                 statistics=tt, p.values=pvals, \n                 indiv.p.values=ipvals))\n}\n\n\n\n# based on code from cherry::closed()\n# removes the (K-1)-element hypothesis set\nlibrary(bitops)\n.bit2boolean <- function (x, N) \n{\n  base <- 2^(1:N - 1)\n  bitAnd(x, base) != 0\n}\np.adjust.closed <- function (test, hypotheses, ...) \n  # test- function that performs the local test. The function should accept a \n  #       subvector of the hypotheses argument as input, and return a p-value.\n  # hypotheses - identifiers of the collection of elementary hypotheses. \n  # ... - additional parameters to the 'test' function\n  # returns adjusted p-values for each hypothesis\n{\n  N <- length(hypotheses)\n  Nmax <- log2(.Machine$integer.max + 1)\n  if (N > Nmax) \n    stop(\"no more than \", Nmax, \" hypotheses supported in full closed testing.\\n Use a shortcut-based test.\")\n  closure <- 1:(2^N - 1)\n  base <- 2^(1:N - 1)\n  offspring <- function(x) {\n    res <- bitAnd(x, closure)\n    res[res != 0]\n  }\n  lengths <- rowSums(sapply(base, function(bs) bitAnd(closure, bs) != 0))\n  \n  idx <- sort.list(lengths, decreasing = TRUE)\n  closure <- closure[idx]\n  lengths <- lengths[idx]\n  closure <- closure[lengths != (N-1)]\n  \n  adjusted <- numeric(2^N - 1)\n  for (i in closure) {\n    if (adjusted[i] < 1) {\n      localtest <- test(hypotheses[.bit2boolean(i,N)], ...)\n      if (localtest > adjusted[i]) {\n        offs <- offspring(i)\n        adjusted[offs] <- pmax(adjusted[offs], localtest)\n      }\n    }\n  }\n  \n  out <- adjusted[base]\n  names(out) <- hypotheses\n  return(out)\n}\n\n\np.adjust.closed.details <- function (test, hypotheses, ...) \n  # returns details for each tested set\n{\n  N <- length(hypotheses)\n  Nmax <- log2(.Machine$integer.max + 1)\n  if (N > Nmax) \n    stop(\"no more than \", Nmax, \" hypotheses supported in full closed testing.\\n Use a shortcut-based test.\")\n  closure <- 1:(2^N - 1)\n  base <- 2^(1:N - 1)\n  offspring <- function(x) {\n    res <- bitAnd(x, closure)\n    res[res != 0]\n  }\n  lengths <- rowSums(sapply(base, function(bs) bitAnd(closure, bs) != 0))\n  \n  idx <- sort.list(lengths, decreasing = TRUE)\n  closure <- closure[idx]\n  lengths <- lengths[idx]\n  closure <- closure[lengths != (N-1)]\n  \n  adjusted <- numeric(2^N - 1)\n  unadjusted <- numeric(2^N - 1)\n  for (i in closure) {\n    unadjusted[i] <- test(hypotheses[.bit2boolean(i,N)], ...)\n    if (adjusted[i] < 1) {\n      localtest <- unadjusted[i]\n      if (localtest > adjusted[i]) {\n        offs <- offspring(i)\n        adjusted[offs] <- pmax(adjusted[offs], localtest)\n      }\n    }\n  }\n  \n  out <- adjusted[base]\n  names(out) <- hypotheses\n  return(list(p.adj=out, closure=closure, unadjusted=unadjusted, adjusted=adjusted))\n}\n\n## data\nstrk <- data.matrix(read.delim(\"StrokeData.txt\", row.names=1))\ncolnames(strk) <- gsub(\"X\", \"\", colnames(strk))\n\nstrk.data <- as.data.frame.table(strk)\nnames(strk.data) <- c(\"Type\", \"Year\", \"Freq\")\nstrk.data <- group_by(strk.data, Year) %>% mutate(Prop = Freq / sum(Freq))\nstrk.data$Year <- as.numeric(as.character(strk.data$Year))\n\nstrk.data2 <- strk.data[rep(1:nrow(strk.data), strk.data$Freq), ]\nstrk.data2$Freq <- NULL; rownames(strk.data2) <- NULL\n",
    "created" : 1461860366708.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3585653875",
    "id" : "5E4BB044",
    "lastKnownWriteTime" : 1457221327,
    "path" : "Z:/EOGeorge/MultiTrend/MultiCAfunctions.R",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}